# streamlit run app.py
# Front-end UI for the HR tool (light-weight dashboard for testing)
import streamlit as st
import numpy as np
import faiss
import json
import os
from pathlib import Path
from io import BytesIO
from data_extraction_utils import clean_text, get_embedding
import fitz  # PyMuPDF for in-memory PDF
from config import DB_DIR, FAISS_INDEX_PATH, ID_MAP_PATH

@st.cache(allow_output_mutation=True)
def load_faiss_index(index_path: str):
    """
    Load and return a FAISS index, using a working-directory hack to support Unicode paths on Windows.
    """
    idx_path = Path(index_path)
    parent = idx_path.parent
    name = idx_path.name
    # Temporarily chdir into parent to avoid Windows Unicode fopen issues
    cwd = os.getcwd()
    os.chdir(str(parent))
    index = faiss.read_index(name)
    os.chdir(cwd)
    return index

@st.cache(allow_output_mutation=True)
def load_id_map(id_map_path: str):
    """
    Load and return the integerâ†’metadata JSON map.
    """
    with open(id_map_path, 'r', encoding='utf-8') as f:
        return json.load(f)

# Load resources once
index = load_faiss_index(str(FAISS_INDEX_PATH))
id_map = load_id_map(str(ID_MAP_PATH))

# Helper: extract text from PDF bytes, preserving Japanese

def extract_text_from_bytes(raw_bytes: bytes) -> str:
    """
    Extract text from PDF content in bytes using PyMuPDF.
    """
    doc = fitz.open(stream=raw_bytes, filetype="pdf")
    full_text = "".join(page.get_text() for page in doc)
    return full_text

# Similarity search

def find_similar(text: str, top_k: int):
    emb = np.array([get_embedding(text)], dtype='float32')
    distances, indices = index.search(emb, top_k)
    results = []
    for dist, idx in zip(distances[0], indices[0]):
        meta = id_map.get(str(int(idx)), {})
        results.append({
            'filename': meta.get('filename', 'unknown'),
            'score': float(1.0 / (1.0 + dist)),
        })
    return results

# Streamlit UI
st.title("ğŸ“„ Resume Similarity Search (æ—¥æœ¬èªå¯¾å¿œ)")
st.markdown("æ—¥æœ¬èªã®è·å‹™çµŒæ­´æ›¸PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€é¡ä¼¼å±¥æ­´æ›¸ã‚’æ¤œç´¢ã—ã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«åã®æ—¥æœ¬èªè¡¨ç¤ºã«ã‚‚å¯¾å¿œã—ã¦ã„ã¾ã™ã€‚")

uploaded_file = st.file_uploader("è·å‹™çµŒæ­´æ›¸PDFã‚’é¸æŠ", type=["pdf"])
top_k = st.slider("è¡¨ç¤ºã™ã‚‹é¡ä¼¼å±¥æ­´æ›¸ã®æ•°", min_value=1, max_value=20, value=5)

if uploaded_file is not None:
    orig_name = uploaded_file.name
    st.write(f"### ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«: {orig_name}")

    raw_bytes = uploaded_file.read()
    try:
        raw_text = extract_text_from_bytes(raw_bytes)
        text = clean_text(raw_text)
        if not text:
            st.error("âš ï¸ PDFã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆãŒæŠ½å‡ºã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚¹ã‚­ãƒ£ãƒ³ç”»åƒã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
        else:
            st.subheader("æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼")
            st.text_area("", text[:1000], height=200)

            with st.spinner("åŸ‹ã‚è¾¼ã¿ã¨æ¤œç´¢ä¸­â€¦"):
                results = find_similar(text, top_k)

            if results:
                st.success(f"ãƒˆãƒƒãƒ—{top_k}ã®é¡ä¼¼å±¥æ­´æ›¸:")
                for i, r in enumerate(results, start=1):
                    st.write(f"**{i}. {r['filename']}** â€” ã‚¹ã‚³ã‚¢: {r['score']:.3f}")
            else:
                st.info("é¡ä¼¼ã™ã‚‹å±¥æ­´æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
    except Exception as e:
        st.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
else:
    st.info("ã¾ãšã¯è·å‹™çµŒæ­´æ›¸PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
